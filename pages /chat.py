import streamlit as st
from langchain.chains import ConversationChain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import tempfile
import os


# Carrega a API Key
api_key = st.secrets["GEMINI"]["api_key"]
llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro-exp-03-25", google_api_key=api_key)
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)


# Configura칞칚o da P치gina
st.set_page_config(page_title="Chatbot Gemini com RAG", page_icon="游뱄", layout="wide")
st.logo("assets/LogoWarrenRena.png", icon_image="assets/LogoWRena.png")
st.title("游뱄 Chatbot Gemini com RAG")


# Inicializa칞칚o do vetorstore no session_state
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None


# Upload do PDF
uploaded_file = st.file_uploader("Carregue um PDF para an치lise", type="pdf")

if uploaded_file is not None:
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name

    # Carrega e processa o PDF
    loader = PyPDFLoader(tmp_file_path)
    pages = loader.load()
    
    # Divide o texto em chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_documents(pages)
    
    # Cria o vetorstore
    st.session_state.vectorstore = FAISS.from_documents(chunks, embeddings)
    
    # Limpa o arquivo tempor치rio
    os.unlink(tmp_file_path)
    st.success("PDF processado com sucesso!")


if "memory" not in st.session_state:   # Inicializa a mem칩ria e a conversa칞칚o no session_state
    st.session_state.memory = ConversationBufferMemory()

if "conversation" not in st.session_state:
    st.session_state.conversation = ConversationChain(llm=llm, memory=st.session_state.memory)


if "messages" not in st.session_state:# Inicializa o hist칩rico no session_state se ainda n칚o existir
    st.session_state["messages"] = []

for message in st.session_state["messages"]:# Exibe o hist칩rico de mensagens no chat
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


user_input = st.chat_input("Digite sua pergunta...")# Campo de input do usu치rio

if user_input:
    
    st.session_state["messages"].append({"role": "user", "content": user_input})# Adiciona a mensagem do usu치rio ao hist칩rico

    
    # Se houver um vetorstore, usa RAG
    if st.session_state.vectorstore:
        # Busca documentos relevantes
        docs = st.session_state.vectorstore.similarity_search(user_input)
        context = "\n".join([doc.page_content for doc in docs])
        
        # Prepara o prompt com contexto
        prompt = f"""Com base no seguinte contexto, responda  pergunta do usu치rio.
        
        Contexto:
        {context}
        
        Pergunta: {user_input}
        
        Resposta:"""
        
        response = st.session_state.conversation.run(prompt)
    else:
        # Resposta sem RAG
        response = st.session_state.conversation.run(user_input)

    
    st.session_state["messages"].append({"role": "assistant", "content": response})# Adiciona a resposta ao hist칩rico

    
    with st.chat_message("assistant"):# Exibe a resposta no chat
        st.markdown(response)
        
